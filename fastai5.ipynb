{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastai5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthakpruthi/deep-learning-fastai/blob/master/fastai5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Tbd8C0wAsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL8h5Zq2mGbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJwlJUzsmCtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09007346-5036-4848-9453-9bc8d1826843"
      },
      "source": [
        "path=untar_data(URLs.MNIST)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-imageclas/mnist_png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMkq714JtKmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b0c30176-e5ed-44f7-a0b8-0adb01afa349"
      },
      "source": [
        "path.ls() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/mnist_sample/labels.csv'),\n",
              " PosixPath('/root/.fastai/data/mnist_sample/valid'),\n",
              " PosixPath('/root/.fastai/data/mnist_sample/train')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUgJZf1UopUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a651e6f0-e1e3-401d-c227-b0c4d883b6ac"
      },
      "source": [
        "  (path/'train').ls()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/mnist_sample/train/3'),\n",
              " PosixPath('/root/.fastai/data/mnist_sample/train/7')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwpsRJRNo9lA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = Config().data_path()/'mnist.pkl.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOYpO8n4ubfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "e82b0cd4-af67-461e-bb3c-8fae619d8dae"
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4e20bcf9cc48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjoin_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPathOrStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPathOrStr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.fastai/data/mnist.pkl.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVyFbVGwud3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=Path('data/mnist')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mBquRgAh3qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " with gzip.open(path/'mnist.pkl.gz','rb') as f:\n",
        "  ((x_train,y_train),(x_valid,y_valid),_)=pickle.load(f,encoding='latin-1')\n",
        "  #underscore is used because we take some random values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DRi8t6ckPZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCfAxdGkkPWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(x_train[0].reshape((28,28)),cmap='gray')\n",
        "x.train.shape\n",
        "#50000,784"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4bFRB2mkPUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#currently they are numpy arrays (we came to know this when we checked x_train.shape (50000,784))and we need them to be tensors\n",
        "x_train,y_train,x_valid,y_valid=map(torch.tensor,(x_train,y_train,x_valid,y_valid))\n",
        "n,c=x_train.shape\n",
        "x_train.shape#torch.Size(50000,784)\n",
        ",y_train.min()#0\n",
        ",y_train.max()#9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLCgdsekng_z",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxwjn7m5-FiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def mse(y_hat,y):\n",
        "  return ((y_hat-y)**2).mean()\n",
        "\n",
        "\n",
        "real\n",
        "x=torch.ones(100,2)\n",
        "x[:,0]=torch.uniform(-1.,1)\n",
        "w=tensor(3.,2)\n",
        "y=x@w+torch.rand(100)#y=w1*x1+w2*x2\n",
        "\n",
        "\n",
        "created\n",
        "w=tensor(-1,1)\n",
        "y_pred=x@a\n",
        "\n",
        "\n",
        "now this was really wrong when we plot the both the graph by plt.plot(x[:,0],y) &plt.plot(x[:,0],y_pred) \n",
        "\n",
        "so we created our own gradient descent as\n",
        "w=nn.Parameter(w)\n",
        "def update()\n",
        "   y_pred=x@a\n",
        "   if(t%10==0):print(mse(y_pred,y))\n",
        "   loss.backward()\n",
        "   with torch.no_grad():\n",
        "      a.sub_(lr*a.grad())\n",
        "      a.grad.zero()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9ioSGx-kPTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "79489bf1-3e7d-4a43-ec1a-63c8de8886da"
      },
      "source": [
        "train_ds=TensorDataset(x_train,y_train)#creates dataset and we get x value and y valur into it\n",
        "valid_ds=TensorDataset(x_valid,y_valid)\n",
        "data=DataBunch.create(train_ds,valid_ds,bs=64)#it will create dataloader and of batch size=64"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-c860e69f0489>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    real\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY5iW6b1kPSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y=next(iter(data.train_dl))\n",
        "x.shape,y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93CZFRHWkPNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here we need to create nn.Module and for this we need to create sub-class for it\n",
        "??Module#do check this \n",
        "class Mnist_Logistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()#this is written so that we get Module features super calls the constructor\n",
        "    self.lin=nn.Linear(784,10,bias=True)#what we want in our model this linear layer that makes the size\n",
        "\n",
        "    def forward(self,xb):return self.lin(xb)#this gives ax+b\n",
        "#this is nothing but a logistic regression model and it is nothig but a neural net with no hidden layers "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YMDx0mUq_b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Mnist_Logistic().cuda()#since we are creating the weight matrices manually we just use cuda to allocate memory to gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5DIYERm6Zak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[p.shape for p in model.parameters()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxCzFiBh6ZXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func=nn.CrossEntropyLoss()#here we can't use mse bcoz we can't be wrong here saying it was 4 instead of 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz3ovF8g6ZVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update(x,y,lr):\n",
        "  wd=1e-5\n",
        "  y_hat=model(x)\n",
        "  w2=0.\n",
        "  for p in model.parameters(): w2+=(p**2).sum()\n",
        "  loss=loss_func(y_hat,y)+w2*wd#we are calling cross entropy\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    for p in model.parameter():#here there is not one weight so we need to loop through the model.parameters \n",
        "      p.sub_(lr*p.grad)\n",
        "      p.grad.zero_()\n",
        "    return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpYi_QVQ6ZSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses=[update(x,y,lr) for x,y in data.train_dl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpbQxsw6-nPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7F0TNBd-nLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now we will make same for neural net instead of Logistic Regression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmxw63in15XG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w=w-lr*(dL/dw)\n",
        "L=mse(m(x,w),y)+wd*w^2    here wd*w^2 is L2 regularization\n",
        "dL/dw=something+2wd*w     here wd*w is called weight decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF_-CE7v-nJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here we have created neural net from scratch \n",
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super()__init__()\n",
        "    self.lin1=nn.Linear(784,50,bias=True)\n",
        "    self.lin2=nn.Linear(50,10,bias=True)#50 because it must match prrevious layer and bcoz our output is 10 in size\n",
        "\n",
        "  def forward(self,xb):\n",
        "    x=self.lin1(xb)\n",
        "    x=F.relu(x)\n",
        "    return self.lin2(x)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRkJa8BTLlVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Mnist_NN().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKskGgsPLlSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses=[update(x,y,lr) for x,y in data.train_dl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uET5duhQLlQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smxyS9ERLlNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def update(x,y,lr):\n",
        "#   wd=1e-5\n",
        "#   y_hat=model(x)\n",
        "#   w2=0.\n",
        "#   for p in model.parameters(): w2+=(p**2).sum()\n",
        "#   loss=loss_func(y_hat,y)+w2*wd#we are calling cross entropy\n",
        "#   loss.backward()\n",
        "#   with torch.no_grad():\n",
        "#     for p in model.parameter():#here there is not one weight so we need to loop through the model.parameters \n",
        "#       p.sub_(lr*p.grad)\n",
        "#       p.grad.zero_()\n",
        "#     return loss.item()\n",
        "def update(x,y,lr):\n",
        "  opt=optim.Adam(model.parameters(),lr)\n",
        "  y_hat=model(x)\n",
        "  loss=loss_func(y_hat,y)\n",
        "  loss.backward()\n",
        "  opt.step()#instead of p.sub\n",
        "  opt.zero_grad()\n",
        "  return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6K8BXkzTrjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses=[update(x,y,1e-3) for x,y in data.train_dl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9AvzrUqT41o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPTo2JkT4yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn=Learner(data,Mnist_NN(),loss_func=loss_func,metrics=accuracy)\n",
        "#we give learner that here is our databunch and here is our py torch and nn.Module instance, loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky8FRoUzT4xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi97GVxkT4r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn,fit_one_cycle(1,lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbyzlfGzU2Pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_lr(show_moms=True)\n",
        "#gives learning plot and momentum plot\n",
        "#when learning rate is low and momentum is high so we go in same direction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzghHiIFU2LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#when we are doing single label multi class classification, we use softmax as our activation function\n",
        "# and cross entropy as our loss\n",
        "#loss_func=nn.CrossEntropyLoss() it includes softmax activation function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_VFUVZ0U2E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}